#!/usr/bin/env python3
"""
My Daily Feed - æƒ…å ±ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«
å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰æœ€æ–°æƒ…å ±ã‚’å–å¾—ã—ã€HTMLãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
è‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯è‡ªå‹•ã§æ—¥æœ¬èªã«ç¿»è¨³ã•ã‚Œã¾ã™ã€‚
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone, timedelta
from deep_translator import GoogleTranslator
import json
import html
import re
import time

JST = timezone(timedelta(hours=9))

# ===================== ã‚½ãƒ¼ã‚¹å®šç¾© =====================

SOURCES = {
    "ochiai_note": {
        "name": "è½åˆé™½ä¸€",
        "type": "rss",
        "url": "https://note.com/ochyai/rss",
        "emoji": "ğŸ§ ",
        "platform": "note",
    },
    "ochiai_yt": {
        "name": "è½åˆé™½ä¸€",
        "type": "youtube_search",
        "query": "è½åˆé™½ä¸€",
        "emoji": "ğŸ§ ",
        "platform": "YouTube",
        "max_age_days": 90,
    },
    "karpathy_yt": {
        "name": "Andrej Karpathy",
        "type": "rss",
        "url": "https://www.youtube.com/feeds/videos.xml?channel_id=UCXUPKJO5MZQN11PqgIvyuvQ",
        "emoji": "ğŸ¤–",
        "platform": "YouTube",
    },
    "hardfork": {
        "name": "Hard Fork",
        "type": "rss",
        "url": "https://feeds.simplecast.com/l2i9YnTd",
        "emoji": "ğŸ™ï¸",
        "platform": "Podcast",
    },
    "every": {
        "name": "Every",
        "type": "scrape",
        "url": "https://every.to",
        "emoji": "ğŸ“",
        "platform": "Newsletter",
    },
    "moltbook": {
        "name": "Moltbook",
        "type": "scrape",
        "url": "https://www.moltbook.com",
        "emoji": "ğŸ“š",
        "platform": "Community",
    },
    "amodei": {
        "name": "Dario Amodei",
        "type": "scrape",
        "url": "https://darioamodei.com",
        "emoji": "ğŸ›ï¸",
        "platform": "Blog",
    },
    "technium": {
        "name": "Kevin Kelly",
        "type": "rss",
        "url": "https://kk.org/thetechnium/feed/",
        "emoji": "ğŸ”®",
        "platform": "The Technium",
    },
    "tedchiang": {
        "name": "Ted Chiang",
        "type": "scrape",
        "url": "https://www.newyorker.com/contributors/ted-chiang",
        "emoji": "âœï¸",
        "platform": "The New Yorker",
    },
    "wired_jp": {
        "name": "WIRED JAPAN",
        "type": "rss",
        "url": "https://wired.jp/rssfeeder/",
        "emoji": "âš¡",
        "platform": "WIRED",
    },
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
}

# ===================== äºŒåå››ç¯€æ°—ãƒ»ä¸ƒåäºŒå€™ =====================

# 2026å¹´ã®äºŒåå››ç¯€æ°—ã¨ä¸ƒåäºŒå€™ï¼ˆæš¦ç”Ÿæ´»ãƒ»å›½ç«‹å¤©æ–‡å°ãƒ™ãƒ¼ã‚¹ï¼‰
SEKKI_72KOU = [
    # (é–‹å§‹æœˆæ—¥, çµ‚äº†æœˆæ—¥, ç¯€æ°—, å€™å, å€™èª­ã¿, ä¸€è¨€)
    ("01-05", "01-09", "å°å¯’", "èŠ¹ä¹ƒæ „", "ã›ã‚Šã™ãªã‚ã¡ã•ã‹ã†", "èŠ¹ãŒæ°´è¾ºã§åŠ›å¼·ãè‚²ã¡å§‹ã‚ã‚‹é ƒã€‚ä¸ƒè‰ãŒã‚†ã§æ–°å¹´ã®ä½“ã‚’æ•´ãˆã¾ã™"),
    ("01-10", "01-14", "å°å¯’", "æ°´æ³‰å‹•", "ã—ã¿ãšã‚ãŸãŸã‹ã‚’ãµãã‚€", "åœ°ä¸­ã§å‡ã£ãŸæ³‰ã®æ°´ãŒã‚ãšã‹ã«å‹•ãå‡ºã™é ƒã€‚æ˜¥ã®æ°—é…ãŒåœ°ã®åº•ã‹ã‚‰"),
    ("01-15", "01-19", "å°å¯’", "é›‰å§‹é›Š", "ãã˜ã¯ã˜ã‚ã¦ãªã", "é›„ã®é›‰ãŒé³´ãå§‹ã‚ã‚‹é ƒã€‚æ±‚æ„›ã®å£°ãŒå†¬ã®é‡ã«éŸ¿ãã¾ã™"),
    ("01-20", "01-24", "å¤§å¯’", "æ¬¾å†¬è¯", "ãµãã®ã¯ãªã•ã", "è•—ã®è–¹ãŒé›ªã®ä¸‹ã‹ã‚‰ãã£ã¨é¡”ã‚’å‡ºã™é ƒã€‚æ˜¥ä¸€ç•ªã®ä¾¿ã‚Šã§ã™"),
    ("01-25", "01-29", "å¤§å¯’", "æ°´æ²¢è…¹å …", "ã•ã‚ã¿ãšã“ãŠã‚Šã¤ã‚ã‚‹", "æ²¢ã®æ°´ãŒåšãå¼µã‚Šã¤ã‚ã¦å‡ã‚‹é ƒã€‚å¯’ã•ã®åº•ã§ã™ãŒã€å…‰ã¯æ—¥ã”ã¨ã«å¼·ã"),
    ("01-30", "02-03", "å¤§å¯’", "é¶å§‹ä¹³", "ã«ã‚ã¨ã‚Šã¯ã˜ã‚ã¦ã¨ã‚„ã«ã¤ã", "é¶ãŒåµã‚’ç”£ã¿å§‹ã‚ã‚‹é ƒã€‚æ˜¥ã«å‘ã‘ã¦ç”Ÿå‘½ãŒå‹•ãå‡ºã—ã¾ã™"),
    ("02-04", "02-08", "ç«‹æ˜¥", "æ±é¢¨è§£å‡", "ã¯ã‚‹ã‹ãœã“ãŠã‚Šã‚’ã¨ã", "æ˜¥é¢¨ãŒå¹ã„ã¦ã€å‡ã£ã¦ã„ãŸå·ã‚„åœ°é¢ã®æ°·ãŒå°‘ã—ãšã¤è§£ã‘å§‹ã‚ã‚‹é ƒ"),
    ("02-09", "02-13", "ç«‹æ˜¥", "é»„é¶¯çç†", "ã†ãã„ã™ãªã", "é¶¯ãŒå±±é‡Œã§ç¾ã—ã„å£°ã§ã•ãˆãšã‚Šå§‹ã‚ã‚‹é ƒã€‚æ˜¥ã®è¨ªã‚Œã‚’å‘Šã’ã‚‹å£°"),
    ("02-14", "02-18", "ç«‹æ˜¥", "é­šä¸Šæ°·", "ã†ãŠã“ãŠã‚Šã‚’ã„ãšã‚‹", "æ°·ãŒå‰²ã‚Œã¦ã€ãã®éš™é–“ã‹ã‚‰é­šãŒé£›ã³è·³ã­ã‚‹é ƒã€‚æ°´ã®ä¸­ã«ã‚‚æ˜¥ãŒæ¥ã¾ã™"),
    ("02-19", "02-23", "é›¨æ°´", "åœŸè„‰æ½¤èµ·", "ã¤ã¡ã®ã—ã‚‡ã†ã†ã‚‹ãŠã„ãŠã“ã‚‹", "é›ªãŒé›¨ã«å¤‰ã‚ã‚Šã€åœŸãŒæ½¤ã„å§‹ã‚ã‚‹é ƒã€‚å¤§åœ°ãŒç›®ã‚’è¦šã¾ã—ã¾ã™"),
    ("02-24", "02-28", "é›¨æ°´", "éœå§‹é†", "ã‹ã™ã¿ã¯ã˜ã‚ã¦ãŸãªã³ã", "æ˜¥éœãŒãŸãªã³ãå§‹ã‚ã‚‹é ƒã€‚é ãã®æ™¯è‰²ãŒã‚„ã‚ã‚‰ã‹ãã«ã˜ã¿ã¾ã™"),
    ("03-01", "03-05", "é›¨æ°´", "è‰æœ¨èŒå‹•", "ãã†ã‚‚ãã‚ã°ãˆã„ãšã‚‹", "è‰ã‚„æœ¨ã®èŠ½ãŒè†¨ã‚‰ã‚“ã§èŒãˆå§‹ã‚ã‚‹é ƒã€‚ã„ã‚ˆã„ã‚ˆæ˜¥æœ¬ç•ªãŒè¿‘ã¥ãã¾ã™"),
    ("03-06", "03-10", "å•“èŸ„", "èŸ„è™«å•“æˆ¸", "ã™ã”ã‚‚ã‚Šã‚€ã—ã¨ã‚’ã²ã‚‰ã", "å†¬ã”ã‚‚ã‚Šã®è™«ãŒåœŸã®ä¸­ã‹ã‚‰å‡ºã¦ãã‚‹é ƒã€‚å¤§åœ°ãŒç›®è¦šã‚ã¾ã™"),
    ("03-11", "03-15", "å•“èŸ„", "æ¡ƒå§‹ç¬‘", "ã‚‚ã‚‚ã¯ã˜ã‚ã¦ã•ã", "æ¡ƒã®èŠ±ãŒã»ã“ã‚ã³å§‹ã‚ã‚‹é ƒã€‚èŠ±ãŒå’²ãã“ã¨ã‚’ã€Œç¬‘ã†ã€ã¨è¡¨ã™ç¾ã—ã„è¡¨ç¾"),
    ("03-16", "03-20", "å•“èŸ„", "èœè™«åŒ–è¶", "ãªã‚€ã—ã¡ã‚‡ã†ã¨ãªã‚‹", "é’è™«ãŒã•ãªãã‹ã‚‰è¶ã¸ã¨ç”Ÿã¾ã‚Œå¤‰ã‚ã‚‹é ƒã€‚æ˜¥ã®å¤‰å®¹ã§ã™"),
    ("03-21", "03-25", "æ˜¥åˆ†", "é›€å§‹å·£", "ã™ãšã‚ã¯ã˜ã‚ã¦ã™ãã†", "é›€ãŒå·£ã‚’ä½œã‚Šå§‹ã‚ã‚‹é ƒã€‚æ˜¥ã®é™½æ°—ã«èª˜ã‚ã‚Œã¦"),
    ("03-26", "03-30", "æ˜¥åˆ†", "æ¡œå§‹é–‹", "ã•ãã‚‰ã¯ã˜ã‚ã¦ã²ã‚‰ã", "æ¡œã®èŠ±ãŒå’²ãå§‹ã‚ã‚‹é ƒã€‚æ—¥æœ¬ã®æ˜¥ã®è±¡å¾´ã§ã™"),
    ("03-31", "04-04", "æ˜¥åˆ†", "é›·ä¹ƒç™ºå£°", "ã‹ã¿ãªã‚Šã™ãªã‚ã¡ã“ãˆã‚’ã¯ã£ã™", "æ˜¥é›·ãŒé³´ã‚Šå§‹ã‚ã‚‹é ƒã€‚ç©ºæ°—ãŒå†¬ã‹ã‚‰æ˜¥ã¸ã¨å…¥ã‚Œæ›¿ã‚ã‚Šã¾ã™"),
    ("04-05", "04-09", "æ¸…æ˜", "ç„é³¥è‡³", "ã¤ã°ã‚ããŸã‚‹", "ç‡•ãŒå—ã®å›½ã‹ã‚‰æ¸¡ã£ã¦ãã‚‹é ƒã€‚æ˜¥ã®ä½¿è€…ã®åˆ°æ¥ã§ã™"),
    ("04-10", "04-14", "æ¸…æ˜", "é´»é›åŒ—", "ã“ã†ãŒã‚“ããŸã¸ã‹ãˆã‚‹", "é›ãŒåŒ—ã¸å¸°ã£ã¦ã„ãé ƒã€‚ç§‹ã«æ¥ãŸæ¸¡ã‚Šé³¥ã¨ã®åˆ¥ã‚Œã®å­£ç¯€"),
    ("04-15", "04-19", "æ¸…æ˜", "è™¹å§‹è¦‹", "ã«ã˜ã¯ã˜ã‚ã¦ã‚ã‚‰ã‚ã‚‹", "æ˜¥ã®é›¨ä¸ŠãŒã‚Šã«è™¹ãŒè¦‹ãˆå§‹ã‚ã‚‹é ƒã€‚ç©ºæ°—ãŒæ½¤ã¿ã¾ã™"),
    ("04-20", "04-24", "ç©€é›¨", "è‘­å§‹ç”Ÿ", "ã‚ã—ã¯ã˜ã‚ã¦ã—ã‚‡ã†ãš", "è‘¦ãŒèŠ½å¹ãå§‹ã‚ã‚‹é ƒã€‚æ°´è¾ºã«ç·‘ãŒæˆ»ã‚Šã¾ã™"),
    ("04-25", "04-29", "ç©€é›¨", "éœœæ­¢å‡ºè‹—", "ã—ã‚‚ã‚„ã‚“ã§ãªãˆã„ãšã‚‹", "éœœãŒé™ã‚Šãªããªã‚Šã€è‹—ãŒè‚²ã¤é ƒã€‚ç”°æ¤ãˆã®æº–å‚™ãŒå§‹ã¾ã‚Šã¾ã™"),
    ("04-30", "05-05", "ç©€é›¨", "ç‰¡ä¸¹è¯", "ã¼ãŸã‚“ã¯ãªã•ã", "ç‰¡ä¸¹ã®èŠ±ãŒå’²ãé ƒã€‚ç™¾èŠ±ã®ç‹ã¨å‘¼ã°ã‚Œã‚‹è¯ã‚„ã‹ã•"),
    ("05-06", "05-10", "ç«‹å¤", "è›™å§‹é³´", "ã‹ã‚ãšã¯ã˜ã‚ã¦ãªã", "è›™ãŒé³´ãå§‹ã‚ã‚‹é ƒã€‚ç”°ã‚“ã¼ã«å…ƒæ°—ãªåˆå”±ãŒéŸ¿ãã¾ã™"),
    ("05-11", "05-15", "ç«‹å¤", "èš¯èš“å‡º", "ã¿ã¿ãšã„ãšã‚‹", "ãƒŸãƒŸã‚ºãŒåœ°ä¸Šã«å‡ºã¦ãã‚‹é ƒã€‚å¤§åœ°ã®æµã¿ã‚’æ”¯ãˆã‚‹ç”Ÿãã‚‚ã®"),
    ("05-16", "05-20", "ç«‹å¤", "ç«¹ç¬‹ç”Ÿ", "ãŸã‘ã®ã“ã—ã‚‡ã†ãš", "ç­ãŒç”Ÿãˆã¦ãã‚‹é ƒã€‚ç«¹æ—ã«æ—¬ã®å‘³è¦šãŒå®Ÿã‚Šã¾ã™"),
    ("05-21", "05-25", "å°æº€", "èš•èµ·é£Ÿæ¡‘", "ã‹ã„ã“ãŠãã¦ãã‚ã‚’ã¯ã‚€", "èš•ãŒæ¡‘ã®è‘‰ã‚’ç››ã‚“ã«é£Ÿã¹å§‹ã‚ã‚‹é ƒ"),
    ("05-26", "05-30", "å°æº€", "ç´…èŠ±æ „", "ã¹ã«ã°ãªã•ã‹ã†", "ç´…èŠ±ãŒç››ã‚“ã«å’²ãé ƒã€‚é®®ã‚„ã‹ãªé»„è‰²ãŒé‡ã«åºƒãŒã‚Šã¾ã™"),
    ("05-31", "06-05", "å°æº€", "éº¦ç§‹è‡³", "ã‚€ãã®ã¨ãã„ãŸã‚‹", "éº¦ãŒç†Ÿã—ã¦åç©«ã‚’è¿ãˆã‚‹é ƒã€‚åˆå¤ã®é»„é‡‘è‰²"),
    ("06-06", "06-10", "èŠ’ç¨®", "è³è‚ç”Ÿ", "ã‹ã¾ãã‚Šã—ã‚‡ã†ãš", "ã‚«ãƒã‚­ãƒªãŒç”Ÿã¾ã‚Œã‚‹é ƒã€‚å°ã•ãªå‘½ã®å–¶ã¿ãŒå§‹ã¾ã‚Šã¾ã™"),
    ("06-11", "06-15", "èŠ’ç¨®", "è…è‰ç‚ºè›", "ãã•ã‚ŒãŸã‚‹ãã•ã»ãŸã‚‹ã¨ãªã‚‹", "è›ãŒé£›ã³å§‹ã‚ã‚‹é ƒã€‚å¤œã®æ°´è¾ºã«ã‚„ã•ã—ã„å…‰ãŒç¯ã‚Šã¾ã™"),
    ("06-16", "06-20", "èŠ’ç¨®", "æ¢…å­é»„", "ã†ã‚ã®ã¿ãã°ã‚€", "æ¢…ã®å®ŸãŒé»„è‰²ãè‰²ã¥ãé ƒã€‚æ¢…é›¨ã®èªæºã¨ã‚‚ã„ã‚ã‚Œã¾ã™"),
    ("06-21", "06-25", "å¤è‡³", "ä¹ƒæ±æ¯", "ãªã¤ã‹ã‚Œãã•ã‹ã‚‹ã‚‹", "å¤æ¯è‰ãŒæ¯ã‚Œå§‹ã‚ã‚‹é ƒã€‚å¤è‡³ã‚’éãã€é™½ã¯ã‚†ã£ãã‚Šã¨çŸ­ããªã‚Šã¾ã™"),
    ("06-26", "06-30", "å¤è‡³", "è–è’²è¯", "ã‚ã‚„ã‚ã¯ãªã•ã", "è–è’²ã®èŠ±ãŒå’²ãé ƒã€‚é›¨ã«æ¿¡ã‚ŒãŸç´«ãŒç¾ã—ã„"),
    ("07-01", "07-06", "å¤è‡³", "åŠå¤ç”Ÿ", "ã¯ã‚“ã’ã—ã‚‡ã†ãš", "åŠå¤ãŒç”Ÿãˆã‚‹é ƒã€‚ç”°æ¤ãˆã‚’çµ‚ãˆã‚‹ç›®å®‰ã¨ã•ã‚Œã¾ã™"),
    ("07-07", "07-11", "å°æš‘", "æ¸©é¢¨è‡³", "ã‚ã¤ã‹ãœã„ãŸã‚‹", "æ¸©ã‹ã„é¢¨ãŒå¹ãå§‹ã‚ã‚‹é ƒã€‚æœ¬æ ¼çš„ãªå¤ã®åˆ°æ¥ã§ã™"),
    ("07-12", "07-16", "å°æš‘", "è“®å§‹é–‹", "ã¯ã™ã¯ã˜ã‚ã¦ã²ã‚‰ã", "è“®ã®èŠ±ãŒé–‹ãå§‹ã‚ã‚‹é ƒã€‚æ—©æœã®æ± ã«æ¸…ã‚‰ã‹ãªç¾ã—ã•"),
    ("07-17", "07-22", "å°æš‘", "é·¹ä¹ƒå­¦ç¿’", "ãŸã‹ã™ãªã‚ã¡ã‚ã–ã‚’ãªã‚‰ã†", "é·¹ã®å¹¼é³¥ãŒé£›ã¶ã“ã¨ã‚’è¦šãˆã‚‹é ƒ"),
    ("07-23", "07-28", "å¤§æš‘", "æ¡å§‹çµèŠ±", "ãã‚Šã¯ã˜ã‚ã¦ã¯ãªã‚’ã‚€ã™ã¶", "æ¡ã®èŠ±ãŒå®Ÿã‚’çµã³å§‹ã‚ã‚‹é ƒ"),
    ("07-29", "08-02", "å¤§æš‘", "åœŸæ½¤æº½æš‘", "ã¤ã¡ã†ã‚‹ãŠã†ã¦ã‚€ã—ã‚ã¤ã—", "åœŸãŒæ¹¿ã‚Šè’¸ã—æš‘ããªã‚‹é ƒã€‚å¤ã®æš‘ã•ã®ç››ã‚Šã§ã™"),
    ("08-03", "08-07", "å¤§æš‘", "å¤§é›¨æ™‚è¡Œ", "ãŸã„ã†ã¨ãã©ãã«ãµã‚‹", "æ™‚æŠ˜å¤§é›¨ãŒé™ã‚‹é ƒã€‚å¤•ç«‹ãŒæš‘ã•ã‚’å’Œã‚‰ã’ã¾ã™"),
    ("08-08", "08-12", "ç«‹ç§‹", "æ¶¼é¢¨è‡³", "ã™ãšã‹ãœã„ãŸã‚‹", "æ¶¼ã—ã„é¢¨ãŒå¹ãå§‹ã‚ã‚‹é ƒã€‚æš¦ã®ä¸Šã§ã¯ç§‹ã®å§‹ã¾ã‚Š"),
    ("08-13", "08-17", "ç«‹ç§‹", "å¯’è‰é³´", "ã²ãã‚‰ã—ãªã", "ã²ãã‚‰ã—ãŒé³´ãå§‹ã‚ã‚‹é ƒã€‚å¤•æš®ã‚Œã«ã‚‚ã®æ‚²ã—ã„å£°ãŒéŸ¿ãã¾ã™"),
    ("08-18", "08-22", "ç«‹ç§‹", "è’™éœ§å‡é™", "ãµã‹ããã‚Šã¾ã¨ã†", "æ·±ã„éœ§ãŒç«‹ã¡ã“ã‚ã‚‹é ƒã€‚æœæ™©ã«ã©ã“ã‹ç§‹ã®æ°—é…"),
    ("08-23", "08-27", "å‡¦æš‘", "ç¶¿æŸé–‹", "ã‚ãŸã®ã¯ãªã—ã¹ã²ã‚‰ã", "ç¶¿ã®è¼ãŒé–‹ãé ƒã€‚ãµã‚ãµã‚ã®ç¶¿ãŒé¡”ã‚’å‡ºã—ã¾ã™"),
    ("08-28", "09-01", "å‡¦æš‘", "å¤©åœ°å§‹ç²›", "ã¦ã‚“ã¡ã¯ã˜ã‚ã¦ã•ã‚€ã—", "æš‘ã•ãŒã‚ˆã†ã‚„ãåã¾ã‚Šå§‹ã‚ã‚‹é ƒã€‚ç©ºãŒé«˜ããªã‚Šã¾ã™"),
    ("09-02", "09-07", "å‡¦æš‘", "ç¦¾ä¹ƒç™»", "ã“ãã‚‚ã®ã™ãªã‚ã¡ã¿ã®ã‚‹", "ç¨²ãŒå®Ÿã‚‹é ƒã€‚ç”°ã‚“ã¼ãŒé»„é‡‘è‰²ã«è‰²ã¥ãã¾ã™"),
    ("09-08", "09-12", "ç™½éœ²", "è‰éœ²ç™½", "ãã•ã®ã¤ã‚†ã—ã‚ã—", "è‰ã«é™ã‚ŠãŸéœ²ãŒç™½ãå…‰ã‚‹é ƒã€‚æœæ™©ã®å†·ãˆè¾¼ã¿ãŒå¢—ã—ã¾ã™"),
    ("09-13", "09-17", "ç™½éœ²", "é¶ºé´’é³´", "ã›ãã‚Œã„ãªã", "é¶ºé´’ãŒé³´ãå§‹ã‚ã‚‹é ƒã€‚ç§‹ã®æ°—é…ãŒè‰²æ¿ƒããªã‚Šã¾ã™"),
    ("09-18", "09-22", "ç™½éœ²", "ç„é³¥å»", "ã¤ã°ã‚ã•ã‚‹", "ç‡•ãŒå—ã¸å¸°ã£ã¦ã„ãé ƒã€‚æ˜¥ã«æ¥ãŸä½¿è€…ã¨ã®åˆ¥ã‚Œ"),
    ("09-23", "09-27", "ç§‹åˆ†", "é›·ä¹ƒåå£°", "ã‹ã¿ãªã‚Šã™ãªã‚ã¡ã“ãˆã‚’ãŠã•ã‚€", "é›·ãŒé³´ã‚‰ãªããªã‚‹é ƒã€‚ç©ºæ°—ãŒæ¾„ã¿å§‹ã‚ã¾ã™"),
    ("09-28", "10-02", "ç§‹åˆ†", "èŸ„è™«åæˆ¸", "ã‚€ã—ã‹ãã‚Œã¦ã¨ã‚’ãµã•ã", "è™«ãŒåœŸã®ä¸­ã«éš ã‚Œã¦æˆ¸ã‚’ãµã•ãé ƒã€‚å†¬æ”¯åº¦ã®å§‹ã¾ã‚Š"),
    ("10-03", "10-07", "ç§‹åˆ†", "æ°´å§‹æ¶¸", "ã¿ãšã¯ã˜ã‚ã¦ã‹ã‚‹ã‚‹", "ç”°ã®æ°´ã‚’æŠœã„ã¦ç¨²åˆˆã‚Šã®æº–å‚™ã‚’ã™ã‚‹é ƒ"),
    ("10-08", "10-12", "å¯’éœ²", "é´»é›æ¥", "ã“ã†ãŒã‚“ããŸã‚‹", "é›ãŒåŒ—ã‹ã‚‰æ¸¡ã£ã¦ãã‚‹é ƒã€‚ç§‹ã®ç©ºã«é›è¡Œã®åˆ—"),
    ("10-13", "10-17", "å¯’éœ²", "èŠèŠ±é–‹", "ããã®ã¯ãªã²ã‚‰ã", "èŠã®èŠ±ãŒå’²ãå§‹ã‚ã‚‹é ƒã€‚ç§‹ã®å½©ã‚Šã§ã™"),
    ("10-18", "10-22", "å¯’éœ²", "èŸ‹èŸ€åœ¨æˆ¸", "ãã‚Šãã‚Šã™ã¨ã«ã‚ã‚Š", "èŸ‹èŸ€ãŒæˆ¸å£ã§é³´ãé ƒã€‚ç§‹ã®å¤œé•·ã«è™«ã®éŸ³ãŒéŸ¿ãã¾ã™"),
    ("10-23", "10-27", "éœœé™", "éœœå§‹é™", "ã—ã‚‚ã¯ã˜ã‚ã¦ãµã‚‹", "éœœãŒåˆã‚ã¦é™ã‚Šã‚‹é ƒã€‚å†¬ã®è¶³éŸ³ãŒè¿‘ã¥ãã¾ã™"),
    ("10-28", "11-01", "éœœé™", "éœæ™‚æ–½", "ã“ã•ã‚ã¨ãã©ããµã‚‹", "å°é›¨ãŒã—ã¨ã—ã¨ã¨é™ã‚‹é ƒã€‚æ™©ç§‹ã®é™ã‹ãªé›¨"),
    ("11-02", "11-06", "éœœé™", "æ¥“è”¦é»„", "ã‚‚ã¿ã˜ã¤ãŸãã°ã‚€", "ç´…è‘‰ã‚„è”¦ãŒè‰²ã¥ãé ƒã€‚å±±ãŒç‡ƒãˆã‚‹ã‚ˆã†ãªç¾ã—ã•ã«"),
    ("11-07", "11-11", "ç«‹å†¬", "å±±èŒ¶å§‹é–‹", "ã¤ã°ãã¯ã˜ã‚ã¦ã²ã‚‰ã", "å±±èŒ¶èŠ±ãŒå’²ãå§‹ã‚ã‚‹é ƒã€‚å†¬ã®åº­ã«å½©ã‚Šã‚’æ·»ãˆã¾ã™"),
    ("11-12", "11-16", "ç«‹å†¬", "åœ°å§‹å‡", "ã¡ã¯ã˜ã‚ã¦ã“ãŠã‚‹", "å¤§åœ°ãŒå‡ã‚Šå§‹ã‚ã‚‹é ƒã€‚å†¬ãŒæœ¬æ ¼çš„ã«ã‚„ã£ã¦ãã¾ã™"),
    ("11-17", "11-21", "ç«‹å†¬", "é‡‘ç›é¦™", "ãã‚“ã›ã‚“ã‹ã•ã", "æ°´ä»™ã®èŠ±ãŒå’²ãå§‹ã‚ã‚‹é ƒã€‚æ¸…æ¥šãªé¦™ã‚ŠãŒæ¼‚ã„ã¾ã™"),
    ("11-22", "11-26", "å°é›ª", "è™¹è”µä¸è¦‹", "ã«ã˜ã‹ãã‚Œã¦ã¿ãˆãš", "è™¹ã‚’è¦‹ã‹ã‘ãªããªã‚‹é ƒã€‚å†¬ã®ç©ºæ°—ã¯ä¹¾ã„ã¦æ¾„ã‚“ã§ã„ã¾ã™"),
    ("11-27", "12-01", "å°é›ª", "æœ”é¢¨æ‰•è‘‰", "ããŸã‹ãœã“ã®ã¯ã‚’ã¯ã‚‰ã†", "åŒ—é¢¨ãŒæœ¨ã®è‘‰ã‚’å¹ãæ‰•ã†é ƒã€‚å†¬æ¯ã‚Œã®æ™¯è‰²"),
    ("12-02", "12-06", "å°é›ª", "æ©˜å§‹é»„", "ãŸã¡ã°ãªã¯ã˜ã‚ã¦ãã°ã‚€", "æ©˜ã®å®ŸãŒé»„è‰²ãè‰²ã¥ãå§‹ã‚ã‚‹é ƒ"),
    ("12-07", "12-11", "å¤§é›ª", "é–‰å¡æˆå†¬", "ãã‚‰ã•ã‚€ããµã‚†ã¨ãªã‚‹", "ç©ºãŒé‡ãé–‰ã–ã•ã‚Œã€æœ¬æ ¼çš„ãªå†¬ãŒè¨ªã‚Œã‚‹é ƒ"),
    ("12-12", "12-16", "å¤§é›ª", "ç†ŠèŸ„ç©´", "ãã¾ã‚ãªã«ã“ã‚‚ã‚‹", "ç†ŠãŒå†¬çœ ã®ãŸã‚ã«ç©´ã«å…¥ã‚‹é ƒã€‚å±±ã‚‚é™ã‹ã«çœ ã‚Šã«ã¤ãã¾ã™"),
    ("12-17", "12-21", "å¤§é›ª", "é±–é­šç¾¤", "ã•ã‘ã®ã†ãŠã‚€ã‚‰ãŒã‚‹", "é®­ãŒç¾¤ãŒã£ã¦å·ã‚’ä¸Šã‚‹é ƒã€‚å‘½ã‚’ã¤ãªãå£®å¤§ãªæ—…"),
    ("12-22", "12-26", "å†¬è‡³", "ä¹ƒæ±ç”Ÿ", "ãªã¤ã‹ã‚Œãã•ã—ã‚‡ã†ãš", "å¤æ¯è‰ãŒèŠ½ã‚’å‡ºã™é ƒã€‚å†¬è‡³ã‚’éãã€é™½ãŒå°‘ã—ãšã¤é•·ããªã‚Šã¾ã™"),
    ("12-27", "12-31", "å†¬è‡³", "éº‹è§’è§£", "ã•ã‚ã—ã‹ã®ã¤ã®ãŠã¤ã‚‹", "é¹¿ã®è§’ãŒè½ã¡ã‚‹é ƒã€‚æ–°ã—ã„å¹´ã¸ã®æº–å‚™ãŒå§‹ã¾ã‚Šã¾ã™"),
    ("01-01", "01-04", "å†¬è‡³", "é›ªä¸‹å‡ºéº¦", "ã‚†ãã‚ãŸã‚Šã¦ã‚€ãã®ã³ã‚‹", "é›ªã®ä¸‹ã§éº¦ãŒèŠ½ã‚’å‡ºã™é ƒã€‚è¦‹ãˆãªã„ã¨ã“ã‚ã§æ˜¥ã¸ã®æº–å‚™"),
]


def get_seasonal_message():
    """ä»Šæ—¥ã®æ—¥ä»˜ã«å¯¾å¿œã™ã‚‹ä¸ƒåäºŒå€™ã®æƒ…å ±ã‚’è¿”ã™"""
    now = datetime.now(JST)
    month_day = now.strftime("%m-%d")

    for start, end, sekki, kou_name, kou_reading, description in SEKKI_72KOU:
        # å¹´ã‚’ã¾ãŸãã‚±ãƒ¼ã‚¹ï¼ˆ12æœˆâ†’1æœˆï¼‰ã«å¯¾å¿œ
        if start <= end:
            if start <= month_day <= end:
                return sekki, kou_name, kou_reading, description
        else:
            if month_day >= start or month_day <= end:
                return sekki, kou_name, kou_reading, description

    return "ç«‹æ˜¥", "æ±é¢¨è§£å‡", "ã¯ã‚‹ã‹ãœã“ãŠã‚Šã‚’ã¨ã", "æ˜¥é¢¨ãŒå¹ã„ã¦æ°·ãŒè§£ã‘å§‹ã‚ã‚‹é ƒ"


# ===================== ç¿»è¨³ =====================

translator = GoogleTranslator(source="en", target="ja")


def is_english(text):
    """ãƒ†ã‚­ã‚¹ãƒˆãŒè‹±èªã‹ã©ã†ã‹ã‚’ç°¡æ˜“åˆ¤å®š"""
    if not text:
        return False
    ascii_chars = sum(1 for c in text if ord(c) < 128)
    return ascii_chars / max(len(text), 1) > 0.8


def translate_to_japanese(text):
    """è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ï¼ˆç„¡æ–™ã®Google Translateä½¿ç”¨ï¼‰"""
    if not text or not is_english(text):
        return text
    try:
        if len(text) > 4500:
            text = text[:4500]
        result = translator.translate(text)
        return result if result else text
    except Exception as e:
        print(f"    âš ï¸ ç¿»è¨³ã‚¹ã‚­ãƒƒãƒ—: {str(e)[:50]}")
        return text


# ===================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====================

def time_ago(dt):
    """æ—¥æ™‚ã‚’ã€Œâ—¯æ™‚é–“å‰ã€ã€Œâ—¯æ—¥å‰ã€ãªã©ã«å¤‰æ›"""
    if dt is None:
        return ""
    now = datetime.now(timezone.utc)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    diff = now - dt
    hours = diff.total_seconds() / 3600
    if hours < 1:
        return "ãŸã£ãŸä»Š"
    elif hours < 24:
        return f"{int(hours)}æ™‚é–“å‰"
    elif hours < 48:
        return "1æ—¥å‰"
    elif hours < 168:
        return f"{int(hours / 24)}æ—¥å‰"
    else:
        weeks = int(hours / 168)
        if weeks <= 4:
            return f"{weeks}é€±é–“å‰"
        else:
            months = int(hours / 720)
            return f"{months}ãƒ¶æœˆå‰"


def clean_html(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«"""
    if not text:
        return ""
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=" ", strip=True)[:300]


def parse_date(entry):
    """feedparserã®entryã‹ã‚‰æ—¥æ™‚ã‚’å–å¾—"""
    for field in ["published_parsed", "updated_parsed"]:
        t = entry.get(field)
        if t:
            try:
                return datetime(*t[:6], tzinfo=timezone.utc)
            except:
                pass
    return None


def parse_youtube_time(time_text):
    """YouTubeã®ã€Œ3æ™‚é–“å‰ã€ã€Œ2é€±é–“å‰ã€ãªã©ã®ç›¸å¯¾æ™‚é–“ã‚’è§£æ"""
    now = datetime.now(timezone.utc)
    if not time_text:
        return None
    num_match = re.search(r'(\d+)', time_text)
    if not num_match:
        return None
    num = int(num_match.group(1))
    if 'æ™‚é–“' in time_text or 'hour' in time_text:
        return now - timedelta(hours=num)
    elif 'æ—¥' in time_text or 'day' in time_text:
        return now - timedelta(days=num)
    elif 'é€±' in time_text or 'week' in time_text:
        return now - timedelta(weeks=num)
    elif 'ã‹æœˆ' in time_text or 'month' in time_text:
        return now - timedelta(days=num * 30)
    elif 'å¹´' in time_text or 'year' in time_text:
        return now - timedelta(days=num * 365)
    return None


def extract_image_from_entry(entry):
    """RSSã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒURLã‚’å–å¾—"""
    # media:thumbnail
    media_thumb = entry.get("media_thumbnail")
    if media_thumb and isinstance(media_thumb, list) and len(media_thumb) > 0:
        url = media_thumb[0].get("url", "")
        if url:
            return url

    # media:content
    media_content = entry.get("media_content")
    if media_content and isinstance(media_content, list):
        for mc in media_content:
            if mc.get("medium") == "image" or (mc.get("type", "").startswith("image")):
                return mc.get("url", "")

    # enclosure (podcasts often use this)
    enclosures = entry.get("enclosures", [])
    for enc in enclosures:
        if enc.get("type", "").startswith("image"):
            return enc.get("href", enc.get("url", ""))

    # image in feed content / summary
    summary = entry.get("summary", "") or entry.get("content", [{}])[0].get("value", "") if entry.get("content") else ""
    if summary:
        soup = BeautifulSoup(summary, "html.parser")
        img = soup.find("img", src=True)
        if img:
            return img["src"]

    return ""


# ===================== ãƒ‡ãƒ¼ã‚¿å–å¾— =====================

def fetch_rss(source_key, source):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—"""
    items = []
    max_age = source.get("max_age_days")
    try:
        print(f"  ğŸ“¡ RSSå–å¾—ä¸­: {source['name']} ({source['platform']})...")
        feed = feedparser.parse(source["url"])
        for entry in feed.entries[:10]:
            title = entry.get("title", "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰")
            link = entry.get("link", "")
            summary = clean_html(entry.get("summary", entry.get("description", "")))
            pub_date = parse_date(entry)
            image = extract_image_from_entry(entry)

            # YouTubeå‹•ç”»ã®å ´åˆã€ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ç”Ÿæˆ
            if not image and "youtube.com" in link:
                vid_match = re.search(r'v=([^&]+)', link)
                if vid_match:
                    image = f"https://i.ytimg.com/vi/{vid_match.group(1)}/mqdefault.jpg"

            if max_age and pub_date:
                age_days = (datetime.now(timezone.utc) - pub_date).days
                if age_days > max_age:
                    continue

            if len(items) >= 5:
                break

            items.append({
                "source_key": source_key,
                "title": title,
                "summary": summary,
                "link": link,
                "date": pub_date,
                "time_ago": time_ago(pub_date),
                "image": image,
                "original_lang": "en" if is_english(title) else "ja",
            })
        print(f"  âœ… {len(items)}ä»¶å–å¾—")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    return items


def search_youtube(source_key, source):
    """YouTubeæ¤œç´¢ã§å‹•ç”»ã‚’å–å¾—"""
    items = []
    query = source.get("query", "")
    max_age = source.get("max_age_days", 90)
    try:
        print(f"  ğŸ” YouTubeæ¤œç´¢ä¸­: {query}...")
        encoded_query = requests.utils.quote(query)
        url = f"https://www.youtube.com/results?search_query={encoded_query}&sp=CAI%3D"
        resp = requests.get(url, headers=HEADERS, timeout=15)

        match = re.search(r'ytInitialData\s*=\s*({.*?});</script>', resp.text, re.DOTALL)
        if not match:
            print("  âš ï¸ YouTubeæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return items

        data = json.loads(match.group(1))

        try:
            contents = data['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']
            for section in contents:
                video_items = section.get('itemSectionRenderer', {}).get('contents', [])
                for vi in video_items:
                    vr = vi.get('videoRenderer')
                    if not vr:
                        continue
                    vid = vr.get('videoId', '')
                    title = vr.get('title', {}).get('runs', [{}])[0].get('text', '')
                    channel = vr.get('ownerText', {}).get('runs', [{}])[0].get('text', '')
                    published = vr.get('publishedTimeText', {}).get('simpleText', '')
                    # ã‚µãƒ ãƒã‚¤ãƒ«å–å¾—
                    thumbs = vr.get('thumbnail', {}).get('thumbnails', [])
                    thumb_url = thumbs[-1].get('url', '') if thumbs else ''
                    if not thumb_url and vid:
                        thumb_url = f"https://i.ytimg.com/vi/{vid}/mqdefault.jpg"

                    if not title or not vid:
                        continue
                    pub_date = parse_youtube_time(published)
                    if pub_date and max_age:
                        age_days = (datetime.now(timezone.utc) - pub_date).days
                        if age_days > max_age:
                            continue

                    items.append({
                        "source_key": source_key,
                        "title": title,
                        "summary": f"ğŸ“º {channel}" if channel else "",
                        "link": f"https://www.youtube.com/watch?v={vid}",
                        "date": pub_date,
                        "time_ago": time_ago(pub_date) if pub_date else published,
                        "image": thumb_url,
                        "original_lang": "ja",
                    })
                    if len(items) >= 5:
                        break
                if len(items) >= 5:
                    break
        except (KeyError, IndexError) as e:
            print(f"  âš ï¸ YouTubeæ¤œç´¢ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}")

        print(f"  âœ… {len(items)}ä»¶å–å¾—")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    return items


def scrape_every():
    """Every.toã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
    items = []
    seen_titles = set()
    skip_words = ["subscribe", "sign up", "log in", "newsletter", "cookie",
                  "introducing every", "pricing", "about", "advertise",
                  "view all", "read more", "careers", "contact"]
    try:
        print(f"  ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: Every...")
        resp = requests.get("https://every.to", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")

        candidates = []
        for heading in soup.find_all(["h1", "h2", "h3"]):
            a_tag = heading.find("a", href=True)
            if a_tag:
                candidates.append(a_tag)

        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            if re.match(r"^/[a-z-]+/[a-z0-9-]+", href) and len(href) > 15:
                candidates.append(a_tag)

        for a_tag in candidates:
            href = a_tag.get("href", "")
            if not href or href.startswith("#"):
                continue
            title_text = ""
            for child in a_tag.descendants:
                if isinstance(child, str):
                    title_text += child
            title_text = title_text.strip()
            title_text = re.sub(r'(?<=[A-Z])\s+(?=[a-z])', '', title_text)
            title_text = re.sub(r'\s+', ' ', title_text).strip()

            if not title_text or len(title_text) < 10 or len(title_text) > 200:
                continue
            if any(sw in title_text.lower() for sw in skip_words):
                continue
            title_lower = title_text.lower()
            if title_lower in seen_titles:
                continue
            seen_titles.add(title_lower)

            # è¿‘ãã®ç”»åƒã‚’æ¢ã™
            image = ""
            parent = a_tag.parent
            for _ in range(5):
                if parent is None:
                    break
                img = parent.find("img", src=True)
                if img and not img["src"].startswith("data:"):
                    image = img["src"]
                    break
                parent = parent.parent

            full_url = href if href.startswith("http") else f"https://every.to{href}"
            items.append({
                "source_key": "every",
                "title": title_text,
                "summary": "",
                "link": full_url,
                "date": None,
                "time_ago": "æœ€è¿‘",
                "image": image,
                "original_lang": "en",
            })
            if len(items) >= 5:
                break
        print(f"  âœ… {len(items)}ä»¶å–å¾—")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    return items


def scrape_moltbook():
    """Moltbookã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    items = []
    try:
        print(f"  ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: Moltbook...")
        requests.get("https://www.moltbook.com", headers=HEADERS, timeout=15)
        items.append({
            "source_key": "moltbook",
            "title": "Moltbook â€” AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒ™ãƒ¼ã‚¿ç‰ˆï¼‰",
            "summary": "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŒå£«ãŒäº¤æµã™ã‚‹æ–°ã—ã„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€‚ãƒ™ãƒ¼ã‚¿ç‰ˆã®ãŸã‚æŠ•ç¨¿ã¯ã¾ã å°‘ãªã‚ã§ã™ã€‚",
            "link": "https://www.moltbook.com",
            "date": None,
            "time_ago": "",
            "image": "",
            "original_lang": "ja",
        })
        print(f"  âœ… {len(items)}ä»¶å–å¾—")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    return items


def scrape_amodei():
    """Dario Amodeiã®å€‹äººãƒ–ãƒ­ã‚°ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
    items = []
    try:
        print(f"  ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: Dario Amodei...")
        resp = requests.get("https://darioamodei.com", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")

        # ãƒ–ãƒ­ã‚°ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            title_text = a_tag.get_text(strip=True)

            if not title_text or len(title_text) < 10 or len(title_text) > 300:
                continue

            skip = ["home", "about", "contact", "subscribe", "menu", "navigation",
                    "dario amodei", "privacy", "terms"]
            if any(sw in title_text.lower() for sw in skip):
                continue

            full_url = href if href.startswith("http") else f"https://darioamodei.com{href}"

            # ç”»åƒã‚’æ¢ã™
            image = ""
            parent = a_tag.parent
            for _ in range(4):
                if parent is None:
                    break
                img = parent.find("img", src=True)
                if img and not img["src"].startswith("data:"):
                    img_src = img["src"]
                    if not img_src.startswith("http"):
                        img_src = f"https://darioamodei.com{img_src}"
                    image = img_src
                    break
                parent = parent.parent

            items.append({
                "source_key": "amodei",
                "title": title_text,
                "summary": "",
                "link": full_url,
                "date": None,
                "time_ago": "",
                "image": image,
                "original_lang": "en",
            })
            if len(items) >= 5:
                break

        print(f"  âœ… {len(items)}ä»¶å–å¾—")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    return items


def scrape_tedchiang():
    """Ted Chiangã®New Yorkerè¨˜äº‹ã‚’å–å¾—"""
    items = []
    try:
        print(f"  ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: Ted Chiang (The New Yorker)...")
        resp = requests.get("https://www.newyorker.com/contributors/ted-chiang",
                          headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")

        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            if not re.search(r'/(magazine|culture|tech|news|science)/', href):
                continue

            title_text = a_tag.get_text(strip=True)
            if not title_text or len(title_text) < 10 or len(title_text) > 300:
                continue

            skip = ["subscribe", "sign in", "newsletter", "new yorker", "podcast",
                    "cartoon", "crossword", "goings on"]
            if any(sw in title_text.lower() for sw in skip):
                continue

            full_url = href if href.startswith("http") else f"https://www.newyorker.com{href}"

            # ç”»åƒ
            image = ""
            parent = a_tag.parent
            for _ in range(5):
                if parent is None:
                    break
                img = parent.find("img", src=True)
                if img and not img["src"].startswith("data:"):
                    image = img["src"]
                    break
                parent = parent.parent

            items.append({
                "source_key": "tedchiang",
                "title": title_text,
                "summary": "",
                "link": full_url,
                "date": None,
                "time_ago": "",
                "image": image,
                "original_lang": "en",
            })
            if len(items) >= 5:
                break

        print(f"  âœ… {len(items)}ä»¶å–å¾—")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    return items


# ===================== ç¿»è¨³å‡¦ç† =====================

def translate_items(all_items):
    """è‹±èªã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚µãƒãƒªãƒ¼ã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    en_items = [i for i in all_items if i.get("original_lang") == "en"]
    if not en_items:
        return

    print(f"\nğŸŒ è‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ {len(en_items)}ä»¶ã‚’æ—¥æœ¬èªã«ç¿»è¨³ä¸­...")

    for i, item in enumerate(en_items):
        original_title = item["title"]
        translated_title = translate_to_japanese(original_title)
        if translated_title != original_title:
            item["title_ja"] = translated_title
            item["title_en"] = original_title
            print(f"  âœ… {i+1}/{len(en_items)}: {original_title[:40]}...")
        else:
            item["title_ja"] = None
            item["title_en"] = original_title

        if item["summary"] and is_english(item["summary"]):
            item["summary_ja"] = translate_to_japanese(item["summary"])
        else:
            item["summary_ja"] = item["summary"]

        time.sleep(0.3)

    print(f"  âœ… ç¿»è¨³å®Œäº†")


# ===================== HTMLç”Ÿæˆ =====================

SOURCE_COLORS = {
    "ochiai_note": {"text": "#8B5E3C", "bg": "#FFF5ED", "badge_bg": "#F5E0CE"},
    "ochiai_yt":   {"text": "#8B5E3C", "bg": "#FFF5ED", "badge_bg": "#F5E0CE"},
    "karpathy_yt": {"text": "#3D7A3D", "bg": "#EFF8EF", "badge_bg": "#D4EDDA"},
    "hardfork":    {"text": "#B83B46", "bg": "#FFF0F1", "badge_bg": "#FADCE0"},
    "every":       {"text": "#2E6B96", "bg": "#EDF5FB", "badge_bg": "#D0E5F5"},
    "moltbook":    {"text": "#6B4E8B", "bg": "#F5F0FA", "badge_bg": "#E4D9F2"},
    "amodei":      {"text": "#2D6A4F", "bg": "#EDF7F0", "badge_bg": "#C8E6C9"},
    "technium":    {"text": "#5C6BC0", "bg": "#EDE7F6", "badge_bg": "#D1C4E9"},
    "tedchiang":   {"text": "#6D4C41", "bg": "#EFEBE9", "badge_bg": "#D7CCC8"},
    "wired_jp":    {"text": "#00695C", "bg": "#E0F2F1", "badge_bg": "#B2DFDB"},
}

def generate_html(all_items, output_path):
    """å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰HTMLãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""

    all_items.sort(key=lambda x: x["date"] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)

    now_jst = datetime.now(JST)
    date_str = now_jst.strftime("%Yå¹´%-mæœˆ%-dæ—¥")
    day_names = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
    day_str = day_names[now_jst.weekday()]
    time_str = now_jst.strftime("%-H:%M")
    hour = now_jst.hour

    if hour < 11:
        greeting = "ãŠã¯ã‚ˆã†ã€Matsuco\U0001F44B\U0001F3FB"
    elif hour < 17:
        greeting = "ã“ã‚“ã«ã¡ã¯ã€Matsuco\U0001F44B\U0001F3FB"
    else:
        greeting = "ã“ã‚“ã°ã‚“ã¯ã€Matsuco\U0001F44B\U0001F3FB"

    # äºŒåå››ç¯€æ°—ãƒ»ä¸ƒåäºŒå€™
    sekki, kou_name, kou_reading, seasonal_desc = get_seasonal_message()

    source_counts = {}
    for item in all_items:
        sk = item["source_key"]
        source_counts[sk] = source_counts.get(sk, 0) + 1

    cards_html = ""
    for idx, item in enumerate(all_items):
        src = SOURCES.get(item["source_key"], {})
        is_en = item.get("original_lang") == "en"
        sc = SOURCE_COLORS.get(item["source_key"], {"text": "#888", "bg": "#F8F8F8", "badge_bg": "#EEE"})
        src_text = sc["text"]
        src_bg = sc["bg"]
        src_badge_bg = sc["badge_bg"]

        if is_en and item.get("title_ja"):
            display_title = item["title_ja"]
            original_line = f'<p class="original-text">åŸæ–‡: {html.escape(item.get("title_en", ""))}</p>'
        else:
            display_title = item["title"]
            original_line = ""

        display_summary = item.get("summary_ja", item["summary"]) if is_en else item["summary"]

        title_escaped = html.escape(display_title)
        summary_escaped = html.escape(display_summary)
        link = html.escape(item.get("link", "#"))
        badge_name = src.get("name", "")
        platform = src.get("platform", "")
        time_ago_str = item.get("time_ago", "")

        lang_badge = ""
        if is_en:
            lang_badge = '<span class="lang-badge">ç¿»è¨³</span>'

        # ç”»åƒéƒ¨åˆ†
        image_url = item.get("image", "")
        image_html = ""
        if image_url:
            image_escaped = html.escape(image_url)
            image_html = f'<div class="card-image"><img src="{image_escaped}" alt="" loading="lazy" onerror="this.parentElement.style.display=\'none\'"></div>'

        cards_html += f"""
        <a href="{link}" target="_blank" rel="noopener" class="card" data-source="{item['source_key']}" style="background: {src_bg}">
            {image_html}
            <div class="card-content">
                <div class="card-header">
                    <span class="source-badge" style="color: {src_text}; background: {src_badge_bg}">{badge_name}</span>
                    <span class="meta">{platform}ã€€{time_ago_str}</span>
                </div>
                <h3 class="card-title">{title_escaped}</h3>
                {original_line}
                {"<p class='card-summary'>" + summary_escaped + "</p>" if summary_escaped else ""}
                <div class="card-footer">
                    {lang_badge}
                    <span class="read-more">ã¤ã¥ãã‚’èª­ã‚€ â†’</span>
                </div>
            </div>
        </a>
        """

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒœã‚¿ãƒ³
    filter_buttons = '<button class="filter-btn active" data-filter="all">ãœã‚“ã¶</button>'
    seen_source_names = set()
    for sk, src in SOURCES.items():
        count = source_counts.get(sk, 0)
        if count > 0 and src["name"] not in seen_source_names:
            seen_source_names.add(src["name"])
            sc = SOURCE_COLORS.get(sk, {"text": "#888", "bg": "#F8F8F8", "badge_bg": "#EEE"})
            total = sum(source_counts.get(k, 0) for k, s in SOURCES.items() if s["name"] == src["name"])
            filter_buttons += f'<button class="filter-btn" data-filter="{sk}" data-color="{sc["badge_bg"]}" data-text="{sc["text"]}" style="background: {sc["badge_bg"]}; color: {sc["text"]}">{src["name"]} <span style="opacity:0.6;font-size:10px">{total}</span></button>'

    en_count = sum(1 for i in all_items if i.get("original_lang") == "en")

    html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ã‘ã•ã®æ‰‹å¸– â€” {date_str}</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Serif+JP:wght@300;400;500;600;700&family=Zen+Maru+Gothic:wght@400;500;700&display=swap');

* {{ margin: 0; padding: 0; box-sizing: border-box; }}

body {{
    font-family: "Zen Maru Gothic", "Noto Serif JP", "Hiragino Kaku Gothic ProN", sans-serif;
    background: #FFFFFF;
    color: #3A3A3A;
    line-height: 1.8;
    letter-spacing: 0.03em;
}}

.container {{
    max-width: 640px;
    margin: 0 auto;
    padding: 52px 28px 80px;
}}

/* === ãƒ˜ãƒƒãƒ€ãƒ¼ === */
.header {{
    margin-bottom: 32px;
}}

.greeting {{
    font-size: 26px;
    font-weight: 700;
    color: #2A2A2A;
    line-height: 1.4;
    margin-bottom: 6px;
    font-family: "Noto Serif JP", serif;
}}

.header .date {{
    font-size: 13px;
    color: #999;
    letter-spacing: 0.08em;
}}

/* === å­£ç¯€ã‚«ãƒ¼ãƒ‰ === */
.season-card {{
    background: linear-gradient(135deg, #fafaf5 0%, #f5f0e8 100%);
    border-radius: 14px;
    padding: 20px 24px;
    margin-bottom: 32px;
    border: 1px solid #ece6d8;
}}

.season-sekki {{
    font-size: 11px;
    color: #A08060;
    letter-spacing: 0.1em;
    margin-bottom: 4px;
}}

.season-kou {{
    font-size: 18px;
    font-weight: 600;
    color: #5A4A3A;
    font-family: "Noto Serif JP", serif;
    margin-bottom: 2px;
}}

.season-reading {{
    font-size: 12px;
    color: #B0A090;
    margin-bottom: 8px;
}}

.season-desc {{
    font-size: 13px;
    line-height: 1.7;
    color: #7A6A5A;
}}

/* === çµ±è¨ˆ === */
.stats {{
    display: flex;
    gap: 20px;
    margin-bottom: 20px;
    font-size: 12px;
    color: #AAA;
}}

/* === ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ === */
.filters {{
    display: flex;
    gap: 8px;
    margin-bottom: 36px;
    flex-wrap: wrap;
}}

.filter-btn {{
    padding: 5px 14px;
    border: none;
    border-radius: 20px;
    background: #F5F5F5;
    color: #777;
    font-family: inherit;
    font-size: 12px;
    letter-spacing: 0.04em;
    cursor: pointer;
    transition: all 0.2s;
}}

.filter-btn.active {{
    background: #3A3A3A !important;
    color: #FFF !important;
}}

.filter-btn:hover:not(.active) {{
    opacity: 0.8;
}}

/* === ã‚«ãƒ¼ãƒ‰ === */
.feed {{
    display: flex;
    flex-direction: column;
    gap: 0px;
}}

.card {{
    display: flex;
    text-decoration: none;
    color: inherit;
    padding: 18px 20px;
    margin-bottom: 12px;
    border-radius: 12px;
    border: none;
    transition: transform 0.12s, box-shadow 0.12s;
    gap: 16px;
    align-items: flex-start;
}}

.card:hover {{
    transform: translateY(-2px);
    box-shadow: 0 4px 16px rgba(0,0,0,0.06);
}}

.card-image {{
    flex-shrink: 0;
    width: 100px;
    height: 72px;
    border-radius: 8px;
    overflow: hidden;
    background: rgba(0,0,0,0.04);
}}

.card-image img {{
    width: 100%;
    height: 100%;
    object-fit: cover;
    display: block;
}}

.card-content {{
    flex: 1;
    min-width: 0;
}}

.card-header {{
    display: flex;
    justify-content: space-between;
    align-items: baseline;
    margin-bottom: 4px;
    gap: 12px;
}}

.source-badge {{
    font-size: 10px;
    font-weight: 600;
    letter-spacing: 0.06em;
    padding: 2px 10px;
    border-radius: 12px;
    display: inline-block;
    white-space: nowrap;
}}

.meta {{
    font-size: 11px;
    color: #BBB;
    white-space: nowrap;
}}

.card-title {{
    font-size: 15px;
    font-weight: 500;
    line-height: 1.6;
    margin-bottom: 4px;
    color: #2A2A2A;
    font-family: "Noto Serif JP", serif;
}}

.card-summary {{
    font-size: 13px;
    line-height: 1.7;
    color: #888;
    margin-bottom: 6px;
}}

.original-text {{
    font-size: 11px;
    color: #BBB;
    margin-bottom: 4px;
    line-height: 1.4;
}}

.card-footer {{
    display: flex;
    align-items: center;
    gap: 10px;
}}

.lang-badge {{
    display: inline-block;
    padding: 1px 7px;
    border-radius: 10px;
    font-size: 10px;
    color: #4A7FA5;
    background: #EDF4F8;
    letter-spacing: 0.04em;
}}

.read-more {{
    font-size: 12px;
    color: #BBB;
    margin-left: auto;
}}

/* === ãƒ•ãƒƒã‚¿ãƒ¼ === */
.footer {{
    margin-top: 56px;
    padding-top: 24px;
    text-align: center;
    font-size: 11px;
    color: #CCC;
}}

.footer-dots {{
    letter-spacing: 0.4em;
    margin-bottom: 12px;
    color: #DDD;
}}

/* === ãƒ¢ãƒã‚¤ãƒ« === */
@media (max-width: 600px) {{
    .container {{ padding: 36px 18px 60px; }}
    .greeting {{ font-size: 22px; }}
    .card {{ padding: 14px 16px; margin-bottom: 10px; flex-direction: column; gap: 10px; }}
    .card-image {{ width: 100%; height: 160px; }}
    .card-title {{ font-size: 15px; }}
    .season-card {{ padding: 16px 18px; }}
    .season-kou {{ font-size: 16px; }}
}}
</style>
</head>
<body>
<div class="container">
    <div class="header">
        <div class="greeting">{greeting}</div>
        <div class="date">{date_str}ï¼ˆ{day_str}ï¼‰ã€€{time_str} å–å¾—</div>
    </div>

    <div class="season-card">
        <div class="season-sekki">{sekki}</div>
        <div class="season-kou">{kou_name}</div>
        <div class="season-reading">{kou_reading}</div>
        <div class="season-desc">{seasonal_desc}</div>
    </div>

    <div class="stats">
        <span>{len(all_items)}ä»¶</span>
        <span>{len([sk for sk in source_counts if source_counts[sk] > 0])}ã¤ã®æƒ…å ±æº</span>
        {"<span>" + str(en_count) + "ä»¶ã‚’ç¿»è¨³</span>" if en_count > 0 else ""}
    </div>

    <div class="filters" id="filters">
        {filter_buttons}
    </div>

    <div class="feed" id="feed">
        {cards_html}
    </div>

    <div class="footer">
        <div class="footer-dots">Â· Â· Â·</div>
        <p>ã‘ã•ã®æ‰‹å¸– â€” é™ã‹ã«ã‚ã¤ã‚ã¦ã„ã¾ã™</p>
    </div>
</div>

<script>
// ã‚½ãƒ¼ã‚¹åã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒãƒƒãƒ”ãƒ³ã‚°
const sourceGroups = {{}};
document.querySelectorAll('.filter-btn[data-filter]').forEach(btn => {{
    const f = btn.dataset.filter;
    if (f !== 'all') {{
        if (!sourceGroups[f]) sourceGroups[f] = [f];
    }}
}});
// è½åˆé™½ä¸€ã®noteã¨YTã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
sourceGroups['ochiai_note'] = ['ochiai_note', 'ochiai_yt'];
sourceGroups['ochiai_yt'] = ['ochiai_note', 'ochiai_yt'];

document.getElementById('filters').addEventListener('click', e => {{
    const btn = e.target.closest('.filter-btn');
    if (!btn) return;
    const filter = btn.dataset.filter;
    document.querySelectorAll('.filter-btn').forEach(b => {{
        b.classList.remove('active');
    }});
    btn.classList.add('active');
    const matchKeys = sourceGroups[filter] || [filter];
    document.querySelectorAll('.card').forEach(card => {{
        if (filter === 'all') {{
            card.style.display = '';
        }} else {{
            card.style.display = matchKeys.includes(card.dataset.source) ? '' : 'none';
        }}
    }});
}});
</script>
</body>
</html>"""
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    print(f"\nğŸ‰ HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")


# ===================== ãƒ¡ã‚¤ãƒ³ =====================

def main():
    print("=" * 50)
    print("ğŸ“° My Daily Feed - ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    print(f"â° {datetime.now(JST).strftime('%Y-%m-%d %H:%M')} JST")
    print("=" * 50)

    all_items = []

    for key, source in SOURCES.items():
        if source["type"] == "rss":
            items = fetch_rss(key, source)
            all_items.extend(items)
        elif source["type"] == "youtube_search":
            items = search_youtube(key, source)
            all_items.extend(items)
        elif key == "every":
            items = scrape_every()
            all_items.extend(items)
        elif key == "moltbook":
            items = scrape_moltbook()
            all_items.extend(items)
        elif key == "amodei":
            items = scrape_amodei()
            all_items.extend(items)
        elif key == "tedchiang":
            items = scrape_tedchiang()
            all_items.extend(items)
        time.sleep(0.5)

    print(f"\nğŸ“Š åˆè¨ˆ {len(all_items)} ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—")

    # è‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¿»è¨³
    translate_items(all_items)

    output_path = "index.html"

    generate_html(all_items, output_path)

if __name__ == "__main__":
    main()
